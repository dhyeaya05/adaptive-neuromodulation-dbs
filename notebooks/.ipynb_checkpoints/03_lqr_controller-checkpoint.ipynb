{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: LQR Optimal Controller\n",
    "\n",
    "**Objective:** Design and implement Linear Quadratic Regulator (LQR) for optimal beta suppression\n",
    "\n",
    "**Theory:**\n",
    "- LQR minimizes quadratic cost function: J = ‚à´(x'Qx + u'Ru)dt\n",
    "- Finds optimal control law: u = -Kx\n",
    "- K computed from Algebraic Riccati Equation\n",
    "- Guaranteed stability and optimality\n",
    "\n",
    "**Expected:** 60-70% beta reduction with lower energy consumption than PID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.linalg import solve_continuous_are\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Load baseline data\n",
    "baseline = np.load('../data/simulation_results/baseline_data.npz')\n",
    "time = baseline['time']\n",
    "baseline_beta = baseline['beta_power']\n",
    "mean_beta = baseline['mean_beta_power']\n",
    "\n",
    "# Control objective\n",
    "TARGET_BETA = mean_beta * 0.3\n",
    "\n",
    "print(\"‚úÖ Loaded baseline data\")\n",
    "print(f\"   Mean beta: {mean_beta:.4f}\")\n",
    "print(f\"   Target: {TARGET_BETA:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Brain Model (Same as PID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBrainModel:\n",
    "    \"\"\"\n",
    "    Brain model for controller testing\n",
    "    \"\"\"\n",
    "    def __init__(self, baseline_beta, dt=0.001):\n",
    "        self.baseline_beta = baseline_beta.copy()\n",
    "        self.beta = baseline_beta[0]\n",
    "        self.dt = dt\n",
    "        self.time_idx = 0\n",
    "        \n",
    "    def step(self, stimulation):\n",
    "        \"\"\"Update brain state with stimulation\"\"\"\n",
    "        if self.time_idx >= len(self.baseline_beta):\n",
    "            return self.beta\n",
    "        \n",
    "        # Natural dynamics from baseline\n",
    "        natural = self.baseline_beta[self.time_idx]\n",
    "        \n",
    "        # Stimulation effect (reduces beta)\n",
    "        stim_effect = stimulation * 0.05\n",
    "        \n",
    "        # Update with simple dynamics\n",
    "        self.beta = natural - stim_effect + np.random.randn() * 0.01\n",
    "        self.beta = max(0.01, self.beta)\n",
    "        \n",
    "        self.time_idx += 1\n",
    "        return self.beta\n",
    "    \n",
    "    def reset(self):\n",
    "        self.time_idx = 0\n",
    "        self.beta = self.baseline_beta[0]\n",
    "\n",
    "print(\"‚úÖ Brain model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. System Identification - State-Space Model\n",
    "\n",
    "For LQR, we need a linear state-space model:\n",
    "```\n",
    "dx/dt = Ax + Bu\n",
    "y = Cx\n",
    "```\n",
    "\n",
    "We'll use a simple 2-state model:\n",
    "- State 1: Beta power deviation from target\n",
    "- State 2: Rate of change of beta power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state-space matrices for a simple oscillator with damping\n",
    "# This approximates brain dynamics around the operating point\n",
    "\n",
    "# System parameters\n",
    "omega = 2 * np.pi * 20  # Natural frequency (20 Hz - beta band)\n",
    "zeta = 0.2              # Damping ratio\n",
    "k_stim = 10.0           # Stimulation effectiveness\n",
    "\n",
    "# State-space matrices\n",
    "# States: x1 = beta_error, x2 = d(beta_error)/dt\n",
    "A = np.array([\n",
    "    [0,              1],\n",
    "    [-omega**2,  -2*zeta*omega]\n",
    "])\n",
    "\n",
    "# Input matrix (stimulation effect)\n",
    "B = np.array([\n",
    "    [0],\n",
    "    [-k_stim]\n",
    "])\n",
    "\n",
    "# Output matrix (we measure beta power)\n",
    "C = np.array([[1, 0]])\n",
    "\n",
    "print(\"üìê State-Space Model:\")\n",
    "print(f\"\\nA (System dynamics) =\\n{A}\")\n",
    "print(f\"\\nB (Input matrix) =\\n{B}\")\n",
    "print(f\"\\nC (Output matrix) =\\n{C}\")\n",
    "\n",
    "# Check controllability\n",
    "controllability_matrix = np.hstack([B, A @ B])\n",
    "rank = np.linalg.matrix_rank(controllability_matrix)\n",
    "print(f\"\\n‚úÖ System is {'controllable' if rank == 2 else 'NOT controllable'}\")\n",
    "print(f\"   Controllability matrix rank: {rank}/2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LQR Controller Design\n",
    "\n",
    "LQR finds optimal gain K that minimizes:\n",
    "```\n",
    "J = ‚à´(x'Qx + u'Ru)dt\n",
    "```\n",
    "\n",
    "Where:\n",
    "- Q: State penalty matrix (penalize deviations from target)\n",
    "- R: Control penalty matrix (penalize high stimulation)\n",
    "- K: Optimal feedback gain (computed from Riccati equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_lqr(A, B, Q, R):\n",
    "    \"\"\"\n",
    "    Design LQR controller\n",
    "    \n",
    "    Returns:\n",
    "        K: Optimal feedback gain\n",
    "        P: Solution to Riccati equation\n",
    "    \"\"\"\n",
    "    # Solve continuous-time algebraic Riccati equation\n",
    "    P = solve_continuous_are(A, B, Q, R)\n",
    "    \n",
    "    # Compute optimal gain\n",
    "    K = np.linalg.inv(R) @ B.T @ P\n",
    "    \n",
    "    return K, P\n",
    "\n",
    "# Design cost matrices\n",
    "# Q: Penalize beta power deviation heavily\n",
    "Q = np.diag([100.0, 1.0])  # High penalty on position, low on velocity\n",
    "\n",
    "# R: Moderate penalty on control effort\n",
    "R = np.array([[0.1]])  # Small value = allow more control\n",
    "\n",
    "# Compute optimal gain\n",
    "K_lqr, P = design_lqr(A, B, Q, R)\n",
    "\n",
    "print(\"üéõÔ∏è LQR Controller Design:\")\n",
    "print(f\"\\nCost matrices:\")\n",
    "print(f\"   Q (state penalty) =\\n{Q}\")\n",
    "print(f\"   R (control penalty) = {R[0,0]}\")\n",
    "print(f\"\\nOptimal Gain:\")\n",
    "print(f\"   K = {K_lqr}\")\n",
    "print(f\"\\nRiccati Solution:\")\n",
    "print(f\"   P =\\n{P}\")\n",
    "\n",
    "# Check closed-loop stability\n",
    "A_cl = A - B @ K_lqr\n",
    "eigenvalues = np.linalg.eigvals(A_cl)\n",
    "print(f\"\\nClosed-loop eigenvalues: {eigenvalues}\")\n",
    "stable = np.all(np.real(eigenvalues) < 0)\n",
    "print(f\"‚úÖ Closed-loop system is {'STABLE' if stable else 'UNSTABLE'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LQR Controller Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LQRController:\n",
    "    \"\"\"\n",
    "    Linear Quadratic Regulator for beta suppression\n",
    "    \"\"\"\n",
    "    def __init__(self, K, dt=0.001):\n",
    "        self.K = K\n",
    "        self.dt = dt\n",
    "        \n",
    "        # State estimate\n",
    "        self.x = np.array([[0.0], [0.0]])  # [beta_error, dbeta/dt]\n",
    "        self.prev_error = 0.0\n",
    "        \n",
    "        # History\n",
    "        self.control_history = []\n",
    "        self.state_history = []\n",
    "        \n",
    "    def compute_control(self, measurement, setpoint):\n",
    "        \"\"\"\n",
    "        Compute optimal control signal\n",
    "        \n",
    "        Args:\n",
    "            measurement: Current beta power\n",
    "            setpoint: Target beta power\n",
    "            \n",
    "        Returns:\n",
    "            Control signal (stimulation)\n",
    "        \"\"\"\n",
    "        # Compute error\n",
    "        error = measurement - setpoint\n",
    "        \n",
    "        # Estimate derivative (finite difference)\n",
    "        derror = (error - self.prev_error) / self.dt\n",
    "        \n",
    "        # Update state estimate\n",
    "        self.x = np.array([[error], [derror]])\n",
    "        \n",
    "        # Optimal control law: u = -Kx\n",
    "        u = -self.K @ self.x\n",
    "        control = float(u[0, 0])\n",
    "        \n",
    "        # Apply saturation\n",
    "        control = np.clip(control, 0.0, 5.0)\n",
    "        \n",
    "        # Update history\n",
    "        self.prev_error = error\n",
    "        self.control_history.append(control)\n",
    "        self.state_history.append(self.x.copy())\n",
    "        \n",
    "        return control\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x = np.array([[0.0], [0.0]])\n",
    "        self.prev_error = 0.0\n",
    "        self.control_history = []\n",
    "        self.state_history = []\n",
    "\n",
    "# Create LQR controller\n",
    "lqr = LQRController(K_lqr, dt=0.001)\n",
    "print(\"‚úÖ LQR controller created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Closed-Loop Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_closed_loop(controller, brain, target, duration_sec=10):\n",
    "    \"\"\"\n",
    "    Run closed-loop control simulation\n",
    "    \"\"\"\n",
    "    dt = 0.001\n",
    "    n_steps = int(duration_sec / dt)\n",
    "    \n",
    "    time_vec = np.zeros(n_steps)\n",
    "    beta_vec = np.zeros(n_steps)\n",
    "    stim_vec = np.zeros(n_steps)\n",
    "    \n",
    "    brain.reset()\n",
    "    controller.reset()\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        current_beta = brain.step(stim_vec[i-1] if i > 0 else 0)\n",
    "        stim = controller.compute_control(current_beta, target)\n",
    "        \n",
    "        time_vec[i] = i * dt\n",
    "        beta_vec[i] = current_beta\n",
    "        stim_vec[i] = stim\n",
    "    \n",
    "    return time_vec, beta_vec, stim_vec\n",
    "\n",
    "print(\"üöÄ Running LQR closed-loop simulation...\")\n",
    "\n",
    "brain_lqr = SimpleBrainModel(baseline_beta)\n",
    "time_lqr, beta_lqr, stim_lqr = run_closed_loop(lqr, brain_lqr, TARGET_BETA, 10.0)\n",
    "\n",
    "# Calculate metrics\n",
    "beta_reduction_lqr = (1 - np.mean(beta_lqr[-5000:]) / mean_beta) * 100\n",
    "energy_lqr = np.sum(stim_lqr**2) * 0.001  # Energy = ‚à´u¬≤dt\n",
    "\n",
    "# Find settling time\n",
    "settling_time_lqr = None\n",
    "tolerance = 0.1 * TARGET_BETA\n",
    "for i, b in enumerate(beta_lqr):\n",
    "    if abs(b - TARGET_BETA) < tolerance:\n",
    "        if np.all(np.abs(beta_lqr[i:i+1000] - TARGET_BETA) < tolerance):\n",
    "            settling_time_lqr = time_lqr[i]\n",
    "            break\n",
    "\n",
    "print(f\"\\n‚úÖ LQR Simulation Complete!\")\n",
    "print(f\"\\nüìä LQR Performance:\")\n",
    "print(f\"   Beta reduction: {beta_reduction_lqr:.1f}%\")\n",
    "print(f\"   Settling time: {settling_time_lqr:.2f}s\" if settling_time_lqr else \"   Settling time: >10s\")\n",
    "print(f\"   Mean stimulation: {np.mean(stim_lqr):.3f} mA\")\n",
    "print(f\"   Energy consumption: {energy_lqr:.2f}\")\n",
    "print(f\"   Max stimulation: {np.max(stim_lqr):.3f} mA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load PID Results for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run PID for fair comparison\n",
    "from src.controllers.pid_controller import PIDController\n",
    "\n",
    "pid = PIDController(kp=2.0, ki=0.5, kd=0.1, dt=0.001)\n",
    "brain_pid = SimpleBrainModel(baseline_beta)\n",
    "\n",
    "time_pid, beta_pid, stim_pid = run_closed_loop(pid, brain_pid, TARGET_BETA, 10.0)\n",
    "\n",
    "beta_reduction_pid = (1 - np.mean(beta_pid[-5000:]) / mean_beta) * 100\n",
    "energy_pid = np.sum(stim_pid**2) * 0.001\n",
    "\n",
    "print(f\"üìä PID Performance (for comparison):\")\n",
    "print(f\"   Beta reduction: {beta_reduction_pid:.1f}%\")\n",
    "print(f\"   Mean stimulation: {np.mean(stim_pid):.3f} mA\")\n",
    "print(f\"   Energy consumption: {energy_pid:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparative Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "\n",
    "# Row 1: Beta power comparison\n",
    "axes[0, 0].plot(time, baseline_beta, 'gray', alpha=0.5, label='Open-Loop', linewidth=2)\n",
    "axes[0, 0].plot(time_pid, beta_pid, 'b-', label='PID', linewidth=2)\n",
    "axes[0, 0].axhline(TARGET_BETA, color='g', linestyle='--', label='Target')\n",
    "axes[0, 0].set_title('PID Controller', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Time (s)')\n",
    "axes[0, 0].set_ylabel('Beta Power')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(time, baseline_beta, 'gray', alpha=0.5, label='Open-Loop', linewidth=2)\n",
    "axes[0, 1].plot(time_lqr, beta_lqr, 'r-', label='LQR', linewidth=2)\n",
    "axes[0, 1].axhline(TARGET_BETA, color='g', linestyle='--', label='Target')\n",
    "axes[0, 1].set_title('LQR Controller', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Time (s)')\n",
    "axes[0, 1].set_ylabel('Beta Power')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Stimulation comparison\n",
    "axes[1, 0].plot(time_pid, stim_pid, 'b-', linewidth=2)\n",
    "axes[1, 0].fill_between(time_pid, 0, stim_pid, alpha=0.3)\n",
    "axes[1, 0].axhline(5.0, color='r', linestyle='--', label='Max Limit')\n",
    "axes[1, 0].set_title('PID Stimulation', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Time (s)')\n",
    "axes[1, 0].set_ylabel('Stimulation (mA)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(time_lqr, stim_lqr, 'r-', linewidth=2)\n",
    "axes[1, 1].fill_between(time_lqr, 0, stim_lqr, alpha=0.3, color='red')\n",
    "axes[1, 1].axhline(5.0, color='r', linestyle='--', label='Max Limit')\n",
    "axes[1, 1].set_title('LQR Stimulation', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Time (s)')\n",
    "axes[1, 1].set_ylabel('Stimulation (mA)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Row 3: Performance comparison\n",
    "# Bar chart\n",
    "metrics = ['Beta\\nReduction (%)', 'Energy\\nConsumption', 'Mean\\nStim (mA)']\n",
    "pid_vals = [beta_reduction_pid, energy_pid/10, np.mean(stim_pid)]\n",
    "lqr_vals = [beta_reduction_lqr, energy_lqr/10, np.mean(stim_lqr)]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[2, 0].bar(x - width/2, pid_vals, width, label='PID', color='blue', alpha=0.7)\n",
    "axes[2, 0].bar(x + width/2, lqr_vals, width, label='LQR', color='red', alpha=0.7)\n",
    "axes[2, 0].set_title('Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[2, 0].set_ylabel('Value')\n",
    "axes[2, 0].set_xticks(x)\n",
    "axes[2, 0].set_xticklabels(metrics)\n",
    "axes[2, 0].legend()\n",
    "axes[2, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Summary table\n",
    "axes[2, 1].axis('off')\n",
    "table_data = [\n",
    "    ['Metric', 'PID', 'LQR', 'Winner'],\n",
    "    ['Beta Reduction', f'{beta_reduction_pid:.1f}%', f'{beta_reduction_lqr:.1f}%', \n",
    "     'LQR' if beta_reduction_lqr > beta_reduction_pid else 'PID'],\n",
    "    ['Energy', f'{energy_pid:.2f}', f'{energy_lqr:.2f}', \n",
    "     'LQR' if energy_lqr < energy_pid else 'PID'],\n",
    "    ['Mean Stim', f'{np.mean(stim_pid):.3f}', f'{np.mean(stim_lqr):.3f}', \n",
    "     'LQR' if np.mean(stim_lqr) < np.mean(stim_pid) else 'PID'],\n",
    "]\n",
    "\n",
    "table = axes[2, 1].table(cellText=table_data, cellLoc='center', loc='center',\n",
    "                        colWidths=[0.3, 0.2, 0.2, 0.2])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Style header row\n",
    "for i in range(4):\n",
    "    table[(0, i)].set_facecolor('#4CAF50')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/simulation_results/pid_vs_lqr_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Comparison plots saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LQR vs PID CONTROLLER COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìä PERFORMANCE METRICS:\")\n",
    "print(f\"\\n{'Metric':<25} {'PID':<15} {'LQR':<15} {'Improvement'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "improvement_beta = ((beta_reduction_lqr - beta_reduction_pid) / beta_reduction_pid) * 100\n",
    "improvement_energy = ((energy_pid - energy_lqr) / energy_pid) * 100\n",
    "\n",
    "print(f\"{'Beta Reduction':<25} {beta_reduction_pid:>6.1f}%{'':<8} {beta_reduction_lqr:>6.1f}%{'':<8} {improvement_beta:>+6.1f}%\")\n",
    "print(f\"{'Energy Consumption':<25} {energy_pid:>6.2f}{'':<9} {energy_lqr:>6.2f}{'':<9} {improvement_energy:>+6.1f}%\")\n",
    "print(f\"{'Mean Stimulation':<25} {np.mean(stim_pid):>6.3f} mA{'':<5} {np.mean(stim_lqr):>6.3f} mA\")\n",
    "\n",
    "print(f\"\\nüèÜ WINNER: {'LQR' if beta_reduction_lqr > beta_reduction_pid else 'PID'}\")\n",
    "\n",
    "print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "if beta_reduction_lqr > beta_reduction_pid:\n",
    "    print(f\"   ‚úÖ LQR achieves {beta_reduction_lqr - beta_reduction_pid:.1f}% better beta suppression\")\n",
    "if energy_lqr < energy_pid:\n",
    "    print(f\"   ‚úÖ LQR uses {improvement_energy:.1f}% less energy\")\n",
    "print(f\"   ‚úÖ LQR guarantees optimal performance for quadratic cost\")\n",
    "print(f\"   ‚úÖ LQR provides systematic tuning (Q/R matrices)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LQR data\n",
    "np.savez('../data/simulation_results/lqr_results.npz',\n",
    "         time=time_lqr,\n",
    "         beta_power=beta_lqr,\n",
    "         stimulation=stim_lqr,\n",
    "         beta_reduction=beta_reduction_lqr,\n",
    "         energy=energy_lqr,\n",
    "         K=K_lqr,\n",
    "         Q=Q,\n",
    "         R=R)\n",
    "\n",
    "print(\"üíæ LQR results saved to: data/simulation_results/lqr_results.npz\")\n",
    "print(\"\\n‚úÖ Notebook 3 Complete!\")\n",
    "print(\"\\nüìà Next: Implement ML enhancement (LSTM state estimator)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
